{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"first app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"OneSentence.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah: 1\n",
      "ah: 27\n",
      "눈: 1\n",
      "감아도: 1\n",
      "느껴지는: 1\n",
      "향기: 1\n",
      "oh: 3\n",
      "은은해서: 1\n",
      "빠져들어: 1\n",
      "저: 3\n",
      "멀리: 1\n",
      "사라진: 1\n",
      "그: 1\n",
      "빛을: 1\n",
      "따라: 1\n",
      "난: 2\n",
      "너에게: 1\n",
      "더: 9\n",
      "다가가: 2\n",
      "날: 3\n",
      "보는: 1\n",
      "네: 8\n",
      "눈빛과: 1\n",
      "비추는: 1\n",
      "빛깔이: 1\n",
      "모든: 4\n",
      "걸: 1\n",
      "멈추게: 1\n",
      "해: 2\n",
      "너를: 4\n",
      "빛나게: 1\n",
      "어느: 1\n",
      "순간: 1\n",
      "내게로: 1\n",
      "조용히: 1\n",
      "스며들어: 1\n",
      "같은: 1\n",
      "꿈을: 1\n",
      "꾸게: 1\n",
      "될: 1\n",
      "테니까: 2\n",
      "넓은: 1\n",
      "세상을: 1\n",
      "눈앞에: 1\n",
      "그려봐: 1\n",
      "진심을: 1\n",
      "느껴봐: 1\n",
      "안에: 2\n",
      "숨겨둔: 1\n",
      "널: 10\n",
      "세상에: 2\n",
      "펼쳐봐: 1\n",
      "있는: 2\n",
      "그대로: 4\n",
      "지금: 2\n",
      "느낌: 2\n",
      "내게: 3\n",
      "보여줘: 2\n",
      "넌: 10\n",
      "나의: 10\n",
      "비올레타: 9\n",
      "나에게: 3\n",
      "다: 6\n",
      "게: 3\n",
      "너: 3\n",
      "나를: 1\n",
      "믿어줄래: 1\n",
      "(줄래): 1\n",
      "어서: 1\n",
      "맘을: 2\n",
      "열어줄래: 1\n",
      "eh: 1\n",
      "신비로운: 1\n",
      "빛깔을: 1\n",
      "담아: 1\n",
      "비춰줄: 1\n",
      "내: 2\n",
      "모습을: 1\n",
      "닮아: 1\n",
      "누구보다: 1\n",
      "빛이: 2\n",
      "수: 2\n",
      "있게: 2\n",
      "너와: 1\n",
      "나: 1\n",
      "함께: 1\n",
      "할: 1\n",
      "아름답게: 1\n",
      "펼쳐진: 1\n",
      "향기로: 1\n",
      "채워진: 1\n",
      "열어: 1\n",
      "네게: 1\n",
      "귀: 1\n",
      "기울여: 1\n",
      "비춰: 2\n",
      "영원히: 1\n",
      "물들여: 1\n",
      "눈부시게: 1\n",
      "빛날: 1\n",
      "닮은: 1\n",
      "하늘에: 3\n",
      "숨겨왔던: 1\n",
      "마음을: 1\n",
      "기다렸던: 1\n",
      "순간을: 1\n",
      "늘: 1\n",
      "바라왔었던: 1\n",
      "담아서: 1\n",
      "언제든지: 1\n",
      "기다릴게: 1\n",
      "항상: 1\n",
      "곁에: 1\n",
      "있으니까: 1\n",
      "화려한: 1\n",
      "조명보다: 1\n",
      "향한: 1\n",
      "빛으로: 1\n",
      "오직: 1\n",
      "위한: 1\n",
      "나니까: 1\n",
      "그려진: 1\n",
      "너의: 1\n",
      "미소가: 1\n",
      "밝게: 1\n",
      "나잖아: 1\n",
      "아름다운: 1\n",
      "맘의: 1\n",
      "flower: 1\n",
      "지켜줄게: 1\n",
      "환히: 1\n",
      "줄게: 1\n",
      "세상: 1\n",
      "빛: 1\n",
      "my: 1\n",
      "heart: 1\n"
     ]
    }
   ],
   "source": [
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[출처](https://blog.tanka.la/2018/09/02/run-your-first-spark-program-using-pyspark-and-jupyter-notebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[출처](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "from datetime import date, timedelta, datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName(\"PysparkExample\")\\\n",
    ".config (\"spark.sql.shuffle.partitions\", \"50\")\\\n",
    ".config(\"spark.driver.maxResultSize\",\"5g\")\\\n",
    ".config (\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_txt = sc.read.text('OneSentence.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                             value|\n",
      "+----------------------------------+\n",
      "|                             Ah ah|\n",
      "|        눈 감아도 느껴지는 향기 oh|\n",
      "|                 은은해서 빠져들어|\n",
      "|    저 멀리 사라진 그 빛을 따라 난|\n",
      "|     너에게 더 다가가 다가가 ah ah|\n",
      "|날 보는 네 눈빛과 날 비추는 빛깔이|\n",
      "|   모든 걸 멈추게 해 너를 더 빛...|\n",
      "|  어느 순간 내게로 조용히 스며들어|\n",
      "|          같은 꿈을 꾸게 될 테니까|\n",
      "|                    저 넓은 세상을|\n",
      "| 네 눈앞에 그려봐 네 진심을 느껴봐|\n",
      "|   네 안에 숨겨둔 널 세상에 펼쳐봐|\n",
      "|  있는 그대로 지금 느낌 그대로 ...|\n",
      "|                  넌 나의 비올레타|\n",
      "|             나에게 다 ah ah ah ah|\n",
      "|                  넌 나의 비올레타|\n",
      "|            모든 게 다 ah ah ah ah|\n",
      "|                  넌 나의 비올레타|\n",
      "|     너 너 너 나를 믿어줄래 (줄래)|\n",
      "|    어서 내게 더 더 더 네 맘을 ...|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_txt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sql(\"DROP TABLE IF EXISTS mst_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sql(\n",
    "\"CREATE TABLE mst_users(\\\n",
    "    user_id         varchar(255)\\\n",
    "  , register_date   varchar(255)\\\n",
    "  , register_device integer\\\n",
    ")\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------------+\n",
      "|user_id|register_date|register_device|\n",
      "+-------+-------------+---------------+\n",
      "+-------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.sql(\"select * from mst_users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input ''U001'' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 1, pos 32)\n\n== SQL ==\nINSERT INTO mst_usersVALUES    ('U001', '2016-08-26', 1)  , ('U002', '2016-08-26', 2)  , ('U003', '2016-08-27', 3)\n--------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\apache-spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input ''U001'' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 1, pos 32)\n\n== SQL ==\nINSERT INTO mst_usersVALUES    ('U001', '2016-08-26', 1)  , ('U002', '2016-08-26', 2)  , ('U003', '2016-08-27', 3)\n--------------------------------^^^\n\r\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:263)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:130)\r\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:76)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e269a8bda9af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'U001'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2016-08-26'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'U002'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2016-08-26'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   , ('U003', '2016-08-27', 3)\")\n\u001b[0m",
      "\u001b[1;32mC:\\apache-spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \"\"\"\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParseException\u001b[0m: \nmismatched input ''U001'' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 1, pos 32)\n\n== SQL ==\nINSERT INTO mst_usersVALUES    ('U001', '2016-08-26', 1)  , ('U002', '2016-08-26', 2)  , ('U003', '2016-08-27', 3)\n--------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "sc.sql(\"INSERT INTO mst_users\\\n",
    "VALUES\\\n",
    "    ('U001', '2016-08-26', 1)\\\n",
    "  , ('U002', '2016-08-26', 2)\\\n",
    "  , ('U003', '2016-08-27', 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
